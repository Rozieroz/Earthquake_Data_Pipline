# -*- coding: utf-8 -*-
"""earthquakes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hP4oR2m8gfDU5Q1V-yx0VG_XDO9PQRQc
"""

import requests
from datetime import datetime, timedelta, UTC
from sqlalchemy import create_engine, text
import _mysql_connector
from pyspark.sql import SparkSession
from pyspark.sql.types import * #imports all datatypes... to use for schema creation
import pandas as pd

import os
from dotenv import load_dotenv

load_dotenv()

spark = SparkSession.builder \
    .appName("Earthquake Data Processing") \
    .config("spark.jars.packages", "mysql:mysql-connector-java:8.0.32") \
    .getOrCreate()

user = os.getenv("MYSQL_USER")
password = os.getenv("MYSQL_PASSWORD")
host = os.getenv("MYSQL_HOST")  # Default to localhost if not set
port = os.getenv("MYSQL_PORT")  # Default to 3306 if not set
database = os.getenv("MYSQL_DB")  # Default to 'earthquake_db' if not set

JDBC_URL = f"jdbc:mysql://{host}:{port}/{database}"

# create table.. mysql requires primary key
def create_db_and_table():

    # create database safely
    MYSQL_URI_NO_DB = f"mysql+mysqlconnector://{user}:{password}@{host}:{port}/"
    engine = create_engine(MYSQL_URI_NO_DB)
    engine_no_db = create_engine(MYSQL_URI_NO_DB, pool_pre_ping=True)

    with engine_no_db.connect() as conn:
        print(f"Creating database if not exists: {database}")
        conn.execute(text(f"CREATE DATABASE IF NOT EXISTS `{database}`;"))

    # reset engine state
    engine_no_db.dispose()


    # connect to db and create table
    MYSQL_URI = f"mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}"

    engine = create_engine(MYSQL_URI, pool_pre_ping=True)
    with engine.connect() as conn:
        conn.execute(text("""
            CREATE TABLE IF NOT EXISTS earthquake_events (
                id VARCHAR(100) PRIMARY KEY,
                place TEXT,
                magnitude DOUBLE,
                time DATETIME,
                longitude DOUBLE,
                latitude DOUBLE,
                depth DOUBLE
            );
        """))

        engine.dispose()
        print("Table 'earthquake_events' updated in MySQL.")

# This script fetches earthquake data from the USGS Earthquake API for the past 1 hr

def fetch_earthquake_data():
    endtime = datetime.now(UTC)
    starttime = endtime - timedelta(minutes=60)

    # Format timestamps in ISO8601 as required by API
    start_str = starttime.strftime("%Y-%m-%dT%H:%M:%S")
    end_str = endtime.strftime("%Y-%m-%dT%H:%M:%S")

    url = (
        "https://earthquake.usgs.gov/fdsnws/event/1/query"
        f"?format=geojson&starttime={start_str}&endtime={end_str}"
    )
    response = requests.get(url)
    response.raise_for_status()  # Raise an error for bad responses
    data = response.json()

    features = data.get("features", [])
    earthquakes = []

    for f in features:
        props = f["properties"]
        coords = f["geometry"]["coordinates"]
        earthquakes.append({
            "id": f["id"],
            "place": props["place"],
            "magnitude": float(props["mag"]) if props["mag"] is not None else None,
            "time": datetime.fromtimestamp(props["time"] / 1000.0),
            "longitude": float(coords[0]),
            "latitude": float(coords[1]),
            "depth": float(coords[2])
        })

    return earthquakes

# Convert to Spark DataFrame
def earthquakes_to_df(data):
    schema = StructType([
        StructField("id", StringType(), False),     #false = not nullable
        StructField("place", StringType(), True),
        StructField("magnitude", DoubleType(), True),
        StructField("time", TimestampType(), True),
        StructField("longitude", DoubleType(), True),
        StructField("latitude", DoubleType(), True),
        StructField("depth", DoubleType(), True)
    ])
    return spark.createDataFrame(data, schema=schema)

# write to mysql with spark
def load_to_mysql(df):
    if df.count() == 0:
        print("No recent earthquakes")
        return

    # write to MySQL
    df.write \
        .format("jdbc") \
        .option("url", f"jdbc:mysql://{host}:{port}/{database}") \
        .option("dbtable", "earthquake_events") \
        .option("user", user) \
        .option("password", password) \
        .option("driver", "com.mysql.cj.jdbc.Driver") \
        .mode("append") \
        .save()
    print("Earthquake data loaded into MySQL.")

earthquakes = fetch_earthquake_data()
df = earthquakes_to_df(earthquakes)
df.show()

"""
    alternative without schema... not recommended, but useful for quick testing:

    earthquakes = fetch_earthquake_data()
    df2 = spark.createDataFrame(earthquakes)  # No schema passed, datatypes are inferred automatically
    df2.show(truncate=False)
    df2.printSchema()
"""



# MYSQL_URI = f"mysql+mysqlconnector://{user}:{password}@{host}:{port}/{database}"

# # Load into MySQL using Pandas
# def load_to_mysql(df):
#     if df.count() == 0:
#         print("No new earthquake data.")
#         return

#     from pyspark.sql.functions import col

#     # Cast all timestamp columns to string before converting to Pandas to avoid type errors
#     df = df.select([
#         col(c).cast("string") if dtype == "timestamp" else col(c)
#         for c, dtype in df.dtypes
#     ])
#     # Convert Spark DataFrame to Pandas DataFrame for SQLAlchemy
#     pdf = df.toPandas()
#     engine = create_engine(MYSQL_URI)
#     pdf.to_sql('earthquake_events', con=engine, if_exists='append', index=False)

if __name__ == "__main__":
    create_db_and_table()

    data = fetch_earthquake_data()

    if not data:
        print("No recent earthquakes.")
        exit()
    else:
        df = earthquakes_to_df(data)
        df.show()

        load_to_mysql(df)